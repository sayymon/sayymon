
São representações numéricas de dados textuais convertidos em [[vetores]], organiza numericamente os dados para que as LLMs encontre padrões.

Para facilitar as buscas por contexto e similaridade através de cálculos como o [[Cosseno]] ou a [[distância euclidiana]] são comumente utilizados em um [[calculo de similaridade]] entre os embeddings para otimizar agrupamentos. 

Além disso, os embeddings também podem ser usados em modelos de aprendizado de máquina para tarefas como classificação, tradução automática e recomendação de conteúdo.

Utilizam os [[Tokens]] que são criados através do processo de [[Tokenização]] que converte texto em numeros.

Embeddings podem representar uma frase inteira e são compostos por tokens que 


Alguns de bancos de dados que suportam embeddings são o [[MySQL]], [[MongoDB]], [[PostgreSQL]], SQLite[[ ]]e [[Elasticsearch]]. Esses bancos de dados permitem armazenar e recuperar embeddings para serem utilizados em diversas aplicações, como processamento de linguagem natural e aprendizado de máquina.

Os embeddings mais utilizados incluem o [[Word2Vec]], [[GloVe]], [[FastText]] e [[BERT]]. Esses modelos são amplamente utilizados em diversas aplicações de processamento de linguagem natural devido à sua eficácia na captura de semântica e contexto das palavras.