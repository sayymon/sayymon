
É uma arquitetura de [[Deep Learning]] que transforma ou altera uma sequencia de entrada em uma nova sequencia de saída 

Foi originado de um artigo de oito cientistas do Google chamado "[Attention is All you Need](https://arxiv.org/pdf/1706.03762v6)" Atenção é tudo o que vc precisa...

Basicamente resolve uma dificuldade dos modelos de NLP que é em correlacionar os textos e capturar as dependências contextuais entre os dados.

Os transformers utilizam de  [[Embeddings]] que são representações numéricas de palavras ou tokens que são utilizadas como entrada para os transformers, ajudam a capturar as relações entre as palavras e a gerar uma saída significativa com base nas entradas fornecidas.

São muito eficientes com conjuntos grandes de dados diferentemente de arquiteturas anteriorres pois focam apenas no que é importantes com o mecanismo de Auto Atenção
### Arquiteturas Anteriores : 

[[RNN]]
[[LSTM]]